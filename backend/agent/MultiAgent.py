# import os
# import asyncio
# from typing import List, Dict, Any, Optional
# from langchain.prompts import ChatPromptTemplate
# from langchain.schema import Document
# from langgraph.graph import StateGraph, END
# from langgraph.prebuilt import ToolExecutor
# from langchain.tools import Tool
# from pydantic import BaseModel, Field
# from typing import Optional, Dict, List, Union
# from pydantic import BaseModel, Field, validator
# from langchain.docstore.document import Document
# from duckduckgo_search import DDGS
# from langchain_nvidia_ai_endpoints import ChatNVIDIA
# from datetime import datetime
# from tensorflow.keras.models import load_model
# from typing import Dict
# from tensorflow.keras.preprocessing import image
# import numpy as np
# from classifier.ShrimpDiseaseClassifier import ShrimpDiseaseClassifier
# import fasttext

# class ConversationTurn(BaseModel):
#     """
#     Represents a single turn in the conversation.
#     """
#     query: str = Field(description="User's query")
#     response: str = Field(description="System's response")
#     timestamp: str = Field(description="ISO format timestamp of the interaction")
#     context: Optional[Dict] = Field(
#         default_factory=dict,
#         description="Additional context from this interaction"
#     )

# class LabeledSearchResult(BaseModel):
#     label: str
#     search_results: List[Document]

# class AgentState(BaseModel):
#     """
#     Comprehensive state management for the multi-agent workflow.
#     """
#     query: str = Field(description="Original user query", default="")

#     image_path: Optional[str] = Field(
#         default=None,
#         description="Path to the uploaded image for analysis"
#     )

#     routing_type: Optional[str] = Field(
#         default=None,
#         description="Type of routing or processing method"
#     )

#     context: List[Document] = Field(
#         default_factory=list,
#         description="Contextual documents retrieved for the query"
#     )

#     search_results: List[Document] = Field(
#         default_factory=list,
#         description="Web search results related to the query"
#     )

#     image_classification_result: Dict[str, float] = Field(
#         default_factory=dict,
#         description="Image classification results with labels and confidence scores"
#     )

#     text_classification_result: Dict[str, float] = Field(
#         default_factory=dict,
#         description="Text classification results with labels and confidence scores (FastText)"
#     )

#     classification_result: Dict[str, float] = Field(
#         default_factory=dict,
#         description="Combined classification results with labels and confidence scores"
#     )

#     database_results: Optional[Union[str, Dict, List]] = Field(
#         default=None,
#         description="Structured results from database search"
#     )

#     final_answer: Optional[str] = Field(
#         default=None,
#         description="Comprehensive answer generated by the agent"
#     )

#     labeled_search_results: Dict[str, List[Document]] = Field(
#         default_factory=dict,
#         description="Search results categorized by classification labels"
#     )

#     conversation_history: List[ConversationTurn] = Field(
#         default_factory=list,
#         description="List of previous conversation turns"
#     )

#     supervisor_decision: Optional[str] = Field(
#         default=None,
#         description="Decision made by the supervisor agent"
#     )

#     max_history: int = Field(
#         default=5,
#         description="Maximum number of conversation turns to keep"
#     )

#     search_requirements: Optional[str] = Field(
#         default=None,
#         description="Requirements for additional search"
#     )

#     additional_search_results: Optional[str] = Field(
#         default=None,
#         description="Results from additional search"
#     )

#     @validator('labeled_search_results', pre=True)
#     def convert_labeled_search_results(cls, v):
#         # If a list of dictionaries is passed, convert it to the expected format
#         if isinstance(v, list):
#             return {item['label']: item['search_results'] for item in v}
#         return v

#     def reset(self):
#         """
#         Reset the state to its default values while preserving conversation history.
#         """
#         # Store current history and max_history
#         current_history = self.conversation_history
#         history_limit = self.max_history

#         # Reset all fields
#         for field in self.__fields__:
#             if field not in ['conversation_history', 'max_history']:
#                 setattr(self, field, self.__fields__[field].default)

#         # Restore history
#         self.conversation_history = current_history
#         self.max_history = history_limit

#     def update_history(self, response: str):
#         """
#         Update conversation history with the current interaction.

#         Args:
#             response (str): System's response to add to history
#         """
#         # Create context dictionary with relevant state information
#         context = {
#             "routing_type": self.routing_type,
#             "has_image": bool(self.image_path),
#             "classification_results": self.classification_result,
#             "database_accessed": bool(self.database_results)
#         }

#         # Create new conversation turn
#         new_turn = ConversationTurn(
#             query=self.query,
#             response=response,
#             timestamp=datetime.now().isoformat(),
#             context=context
#         )

#         # Add to history
#         self.conversation_history.append(new_turn)

#         # Maintain max history limit
#         if len(self.conversation_history) > self.max_history:
#             self.conversation_history = self.conversation_history[-self.max_history:]

#     def get_formatted_history(self) -> str:
#         """
#         Get formatted conversation history for use in prompts.

#         Returns:
#             str: Formatted conversation history
#         """
#         if not self.conversation_history:
#             return "Kh√¥ng c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán."

#         formatted_history = "C√°c cu·ªôc tr√≤ chuy·ªán g·∫ßn ƒë√¢y:\n"
#         for i, turn in enumerate(self.conversation_history, 1):
#             formatted_history += (
#                 f"\n{i}. Th·ªùi gian: {turn.timestamp}\n"
#                 f"   Ng∆∞·ªùi d√πng: {turn.query}\n"
#                 f"   H·ªá th·ªëng: {turn.response[:200]}...\n"
#                 f"   Ng·ªØ c·∫£nh: {self._format_context(turn.context)}\n"
#             )
#         return formatted_history

#     def _format_context(self, context: Dict) -> str:
#         """
#         Format context information for display.

#         Args:
#             context (Dict): Context dictionary from conversation turn

#         Returns:
#             str: Formatted context string
#         """
#         if not context:
#             return "Kh√¥ng c√≥ th√¥ng tin ng·ªØ c·∫£nh"

#         context_parts = []
#         if context.get("routing_type"):
#             context_parts.append(f"Lo·∫°i x·ª≠ l√Ω: {context['routing_type']}")
#         if context.get("has_image"):
#             context_parts.append("C√≥ h√¨nh ·∫£nh")
#         if context.get("classification_results"):
#             results = context["classification_results"]
#             classifications = [f"{label}: {conf:.2f}" for label, conf in results.items()]
#             context_parts.append(f"Ph√¢n lo·∫°i: {', '.join(classifications)}")
#         if context.get("database_accessed"):
#             context_parts.append("ƒê√£ truy c·∫≠p CSDL")

#         return " | ".join(context_parts) if context_parts else "Kh√¥ng c√≥ th√¥ng tin ng·ªØ c·∫£nh"

#     def is_empty(self) -> bool:
#         """
#         Check if the state is essentially empty (excluding history).
#         """
#         return (
#             not self.query and
#             not self.image_path and
#             not self.context and
#             not self.search_results and
#             not self.image_classification_result and
#             not self.text_classification_result and
#             not self.classification_result and
#             not self.database_results and
#             not self.final_answer and
#             not self.labeled_search_results
#         )

# class MultiAgentShrimpRetrievalSystem:
#     def __init__(self,
#                  contextual_retriever,
#                  llm_model: str = "gemini-1.5-pro",
#                  yolo_model_path: str = "./models/yolo/best_yolo.pt",
#                  cnn_model_path: str = "./models/cnn/VGG16.keras",
#                  fasttext_model_path: str = "./models/fasttext/fasttext_model.bin",
#                  max_history: int = 5
#                  ):
        
#         self.llm = ChatNVIDIA(model="meta/llama-3.1-405b-instruct")
#         self.retriever = contextual_retriever
#         # Initialize specialized agents
#         self.semantic_router = self._create_semantic_router()
#         self.supervisor_agent = self._create_supervisor_agent()
#         self.searcher_agent = self._create_searcher_agent()
#         self.classifier_agent = self._create_classifier_agent(cnn_model_path, fasttext_model_path, yolo_model_path)

#         # Create the optimized workflow graph
#         self.workflow = self._create_workflow()

#         # Initialize default state
#         self.default_state = AgentState(max_history=max_history)

# # <==================================CREATE-AGENT==================================>

#     def _create_semantic_router(self):
#         """
#         Create an enhanced semantic router that also detects disease descriptions.
#         """
#         semantic_router_prompt = ChatPromptTemplate.from_template("""
#         B·∫°n l√† m·ªôt b·ªô ƒë·ªãnh tuy·∫øn ng·ªØ nghƒ©a chuy√™n ph√¢n t√≠ch truy v·∫•n li√™n quan ƒë·∫øn b·ªánh t√¥m.

#         L·ªãch s·ª≠ tr√≤ chuy·ªán:
#         {history}

#         image_path = {image_path}

#         Nhi·ªám v·ª•: Ph√¢n t√≠ch truy v·∫•n v√† tr·∫£ v·ªÅ ƒê√öNG M·ªòT t·ª´ kh√≥a:
#         - "disease_image" n·∫øu truy v·∫•n V·ªÄ ·∫£nh b·ªánh t√¥m (image_path != None)
#         - "disease_description" n·∫øu truy v·∫•n M√î T·∫¢ tri·ªáu ch·ª©ng ho·∫∑c d·∫•u hi·ªáu b·ªánh t√¥m
#         - "disease_query" n·∫øu truy v·∫•n H·ªéI V·ªÄ b·ªánh t√¥m nh∆∞ng kh√¥ng c√≥ ·∫£nh b·ªánh t√¥m (image_path == None) v√† kh√¥ng m√¥ t·∫£ tri·ªáu ch·ª©ng
#         - "chitchat" n·∫øu truy v·∫•n KH√îNG li√™n quan ƒë·∫øn b·ªánh t√¥m

#         Truy v·∫•n hi·ªán t·∫°i: {query}

#         C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ch·ªâ ƒë∆∞·ª£c l√† m·ªôt trong ba t·ª´ kh√≥a tr√™n:
#         """)

#         return semantic_router_prompt

#     def _create_supervisor_agent(self):
#         """
#         Create optimized supervisor agent for query analysis.
#         """
#         supervisor_prompt = ChatPromptTemplate.from_template("""
#         B·∫°n l√† m·ªôt gi√°m s√°t vi√™n th√¥ng minh chuy√™n ph√¢n t√≠ch truy v·∫•n v·ªÅ b·ªánh t√¥m.

#         L·ªãch s·ª≠ tr√≤ chuy·ªán:
#         {history}

#         Nhi·ªám v·ª•: Ph√¢n t√≠ch th√¥ng tin v√† quy·∫øt ƒë·ªãnh h∆∞·ªõng x·ª≠ l√Ω ti·∫øp theo.
#         Tr·∫£ v·ªÅ M·ªòT trong c√°c t·ª´ kh√≥a:
#         - "answer" n·∫øu th√¥ng tin ƒê·ª¶ ƒë·ªÉ tr·∫£ l·ªùi
#         - "search" n·∫øu C·∫¶N t√¨m ki·∫øm th√™m

#         Truy v·∫•n: {query}
#         Th√¥ng tin hi·ªán c√≥: {available_info}

#         Quy·∫øt ƒë·ªãnh c·ªßa b·∫°n (ch·ªâ tr·∫£ v·ªÅ m·ªôt t·ª´ 'answer' ho·∫∑c 'search'):
#         """)

#         return supervisor_prompt

#     def _create_searcher_agent(self):
#         """
#         Create optimized searcher agent.
#         """
#         searcher_prompt = ChatPromptTemplate.from_template("""
#         B·∫°n l√† chuy√™n gia t√¨m ki·∫øm th√¥ng tin v·ªÅ b·ªánh t√¥m.

#         Truy v·∫•n: {query}
#         Th√¥ng tin c·∫ßn t√¨m: {search_requirements}

#         T·ªïng h·ª£p th√¥ng tin t√¨m ƒë∆∞·ª£c, t·∫≠p trung v√†o:
#         1. ƒê·ªô ch√≠nh x√°c v√† tin c·∫≠y
#         2. M·ª©c ƒë·ªô li√™n quan v·ªõi truy v·∫•n
#         3. T√≠nh th·ª±c ti·ªÖn trong ƒëi·ªÅu tr·ªã

#         T·ªïng h·ª£p th√¥ng tin:
#         """)

#         return searcher_prompt

#     def _create_classifier_agent(self, cnn_path, fasttext_path, yolo_model_path):
#         """
#         Create optimized classifier agents.
#         """
#         img_classifier = ShrimpDiseaseClassifier(model_path=cnn_path, yolo_model_path=yolo_model_path)
#         text_classifier = fasttext.load_model(fasttext_path)
#         return (img_classifier, text_classifier)

# # <==================================AGENT-WORKFLOW==================================>

#     def _create_workflow(self):
#         """
#         Create optimized workflow with all agents.
#         """
#         workflow = StateGraph(AgentState)

#         # Add nodes
#         workflow.add_node("semantic_router", self._run_semantic_router)
#         workflow.add_node("classifier", self._run_classifier)
#         workflow.add_node("retriever", self._run_retriever)
#         workflow.add_node("supervisor", self._run_supervisor)
#         workflow.add_node("searcher", self._run_searcher)
#         workflow.add_node("answer_generator", self._generate_final_answer)
#         workflow.add_node("chitchat_generator", self._generate_chitchat_answer)

#         # Define optimized routing
#         workflow.add_conditional_edges(
#             "semantic_router",
#             self._route_query_type,
#             {
#                 "disease_image": "classifier",
#                 "disease_description": "classifier",
#                 "disease_query": "retriever",
#                 "chitchat": "chitchat_generator"
#             }
#         )

#         # Define supervisor routing
#         workflow.add_conditional_edges(
#             "supervisor",
#             self._route_supervisor_decision,
#             {
#                 "search": "searcher",
#                 "answer": "answer_generator"
#             }
#         )

#         # Add remaining edges
#         workflow.add_edge("classifier", "retriever")
#         workflow.add_edge("retriever", "supervisor")
#         workflow.add_edge("searcher", "answer_generator")
#         workflow.add_edge("chitchat_generator", END)
#         workflow.add_edge("answer_generator", END)

#         workflow.set_entry_point("semantic_router")
#         return workflow.compile()

#     def _route_query_type(self, state: AgentState):
#         return state.routing_type

#     def _route_supervisor_decision(self, state: AgentState):
#         return state.supervisor_decision

# # <==================================RUN-AGENT==================================>

#     def _run_semantic_router(self, state: AgentState):
#         print(f"üö¶ SEMANTIC ROUTER: Processing at {datetime.now().isoformat()}")
#         semantic_messages = self.semantic_router.format_messages(
#             query=state.query,
#             image_path=state.image_path,
#             history=state.get_formatted_history()
#         )
#         semantic_response = self.llm.invoke(semantic_messages)
#         state.routing_type = semantic_response.content.lower().strip()
#         print(f"Routing type: {state.routing_type}")
#         return state

#     def _run_classifier(self, state: AgentState):
#         print(f"üë®‚Äç‚öïÔ∏è DISEASES ANALYZER: Processing at {datetime.now().isoformat()}")
#         try:
#             img_classifier, text_classifier = self.classifier_agent[0], self.classifier_agent[1]
#             state.classification_result = {}

#             # Process image if provided
#             if state.image_path:
#                 img_results = img_classifier.predict(state.image_path)
#                 if isinstance(img_results, tuple):
#                     img_results = {
#                         label[0]: float(confidence)  # Removed .item()
#                         for label, confidence in zip(img_results[0], img_results[1])
#                     }
#                 state.image_classification_result.update(img_results)

#             # Process text for disease descriptions
#             if state.routing_type == "disease_description":
#                 text_results = text_classifier.predict(state.query)
#                 if isinstance(text_results, tuple):
#                     text_results = {
#                         label.replace('__label__', ''): float(confidence)  # Removed .item()
#                         for label, confidence in zip(text_results[0], text_results[1])
#                     }
#                 state.text_classification_result.update(text_results)

#             print(state.image_classification_result)
#             print(state.text_classification_result)

#             # Combine results
#             combined_results = {}
#             all_labels = set(state.image_classification_result.keys()).union(set(state.text_classification_result.keys()))
#             for label in all_labels:
#                 img_conf = state.image_classification_result.get(label, 0)
#                 text_conf = state.text_classification_result.get(label, 0)

#                 if img_conf > 0 and text_conf > 0:
#                     combined_conf = 0.6 * img_conf + 0.4 * text_conf
#                 elif img_conf > 0:
#                     combined_conf = img_conf
#                 else:
#                     combined_conf = text_conf

#                 combined_results[label] = combined_conf

#             state.classification_result = combined_results
#             print(combined_results)
#             return state

#         except Exception as e:
#             print(f"‚ùå Classification error: {e}")
#             state.classification_result = {}
#             return state

#     def _run_retriever(self, state: AgentState):
#         print(f"üîç RETRIEVER: Processing at {datetime.now().isoformat()}")
#         """
#         Run optimized retrieval process.
#         """
#         try:
#             search_results = []

#             if state.classification_result:
#                 # Use classification results for targeted search
#                 for label, confidence in state.classification_result.items():
#                     query = f"b·ªánh {label} ·ªü t√¥m | tri·ªáu ch·ª©ng | ƒëi·ªÅu tr·ªã"
#                     results = self.retriever.retrieve_documents(query)
#                     search_results.extend(results)
#             else:
#                 # Direct search for general disease queries
#                 results = self.retriever.retrieve_documents(state.query)
#                 search_results.extend(results)

#             state.search_results = search_results
#             # print(state.search_results)
#             return state

#         except Exception as e:
#             print(f"‚ùå Retrieval error: {e}")
#             state.search_results = []
#             return state

#     def _run_supervisor(self, state: AgentState):
#         print(f"üßê SUPERVISOR: Processing at {datetime.now().isoformat()}")
#         """
#         Run supervisor for decision making.
#         """
#         available_info = {
#             "classification": self._format_classification_results(state.classification_result),
#             "search_results": self._format_search_results(state.search_results),
#             "additional_search_results": state.additional_search_results or "Kh√¥ng c√≥ th√¥ng tin b·ªï sung",
#         }

#         supervisor_messages = self.supervisor_agent.format_messages(
#             query=state.query,
#             history=state.get_formatted_history(),
#             available_info=available_info
#         )
#         # print(supervisor_messages)
#         try:
#             response = self.llm.invoke(supervisor_messages)
#             state.supervisor_decision = response.content.lower().strip()
#             print(f"Supervisor decision: {state.supervisor_decision}")
#             return state

#         except Exception as e:
#             print(f"‚ùå Supervisor error: {e}")
#             state.supervisor_decision = "search"  # Default to search on error
#             return state

#     def _run_searcher(self, state: 'AgentState'):
#         print(f"üåê SEARCHER: Processing at {datetime.now().isoformat()}")
#         """
#         Run optimized searcher for additional information.
#         """
#         if not state.search_requirements:
#             state.search_requirements = "th√¥ng tin chung v·ªÅ b·ªánh v√† c√°ch ƒëi·ªÅu tr·ªã"

#         searcher_messages = self.searcher_agent.format_messages(
#             query=state.query,
#             search_requirements=state.search_requirements
#         )

#         max_retries = 3
#         retry_delay = 2  # Initial delay in seconds

#         for attempt in range(max_retries):
#             try:
#                 search_tool = DDGS()
#                 results = [r for r in search_tool.text(state.query, max_results=5)]
#                 state.additional_search_results = "\n".join([str(r) for r in results])
#                 print(f"üîé Web search results for '{state.query}'")
#                 print(state.additional_search_results)
#                 return state

#             except Exception as e:
#                 error_message = str(e)
#                 if "Ratelimit" in error_message:
#                     print(f"‚ùå Search error: {error_message}. Retrying in {retry_delay} seconds...")
#                     time.sleep(retry_delay)
#                     retry_delay *= 2  # Exponential backoff
#                 else:
#                     print(f"‚ùå Search error: {error_message}")
#                     state.additional_search_results = ""
#                     return state

#         print(f"‚ùå Search error: Exceeded maximum retries.")
#         state.additional_search_results = ""
#         return state

#     def _generate_final_answer(self, state: AgentState):
#         print(f"üìã ANSWER GENERATOR: Processing at {datetime.now().isoformat()}")
#         """
#         Generate optimized response.
#         """
#         answer_prompt = ChatPromptTemplate.from_template("""
#         B·∫°n l√† chuy√™n gia t∆∞ v·∫•n v·ªÅ b·ªánh t√¥m.

#         L·ªãch s·ª≠ tr√≤ chuy·ªán:
#         {history}

#         Truy v·∫•n: {query}
#         K·∫øt qu·∫£ ph√¢n lo·∫°i: {classification_results}
#         Th√¥ng tin tham kh·∫£o: {search_results}
#         Th√¥ng tin b·ªï sung: {additional_info}

#         Y√™u c·∫ßu:
#         1. Tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m
#         2. ∆Øu ti√™n th√¥ng tin t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
#         3. N√™u r√µ ngu·ªìn th√¥ng tin n·∫øu c√≥ (N·∫øu ngu·ªìn c√≥ ƒë√≠nh k√®m link ph·∫£i n√™u link ra, N·∫øu ch·ªâ l·∫•y t·ª´ th√¥ng tin tham kh·∫£o th√¨ kh√¥ng c·∫ßn ghi ngu·ªìn)
#         4. K·∫øt n·ªëi v·ªõi th√¥ng tin t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán n·∫øu ph√π h·ª£p
#         5. Di·ªÖn ƒë·∫°t d·ªÖ hi·ªÉu, khoa h·ªçc v√† ch√≠nh x√°c
#         Tr·∫£ l·ªùi:
#         """)

#         messages = answer_prompt.format_messages(
#             query=state.query,
#             history=state.get_formatted_history(),
#             classification_results=self._format_classification_results(state.classification_result),
#             search_results=self._format_search_results(state.search_results),
#             additional_info=state.additional_search_results or "Kh√¥ng c√≥ th√¥ng tin b·ªï sung"
#         )
#         # print(messages)
#         try:
#             response = ""
#             for chunk in self.llm.stream(messages):
#                 if hasattr(chunk, 'content'):
#                     print(chunk.content, end='', flush=True)
#                     response += chunk.content

#             state.final_answer = response.strip()
#             state.update_history(response)
#             return state

#         except Exception as e:
#             print(f"‚ùå Answer generation error: {e}")
#             error_message = "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói h·ªá th·ªëng."
#             state.final_answer = error_message
#             state.update_history(error_message)
#             return state

#     def _generate_chitchat_answer(self, state: AgentState):
#         print(f"üìã ANSWER GENERATOR: ChitChat answer at {datetime.now().isoformat()}")
#         """
#         Generate simple chitchat response.
#         """
#         chitchat_prompt = ChatPromptTemplate.from_template("""
#         B·∫°n l√† chuy√™n gia v·ªÅ b·ªánh t√¥m.
#         Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán cho c√¢u h·ªèi: {query}
#         """)

#         messages = chitchat_prompt.format_messages(query=state.query)

#         try:
#             response = ""
#             for chunk in self.llm.stream(messages):
#                 if hasattr(chunk, 'content'):
#                     print(chunk.content, end='', flush=True)
#                     response += chunk.content

#             state.final_answer = response.strip()
#             state.update_history(response)
#             return state

#         except Exception as e:
#             print(f"‚ùå Chitchat generation error: {e}")
#             error_message = "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói h·ªá th·ªëng."
#             state.final_answer = error_message
#             state.update_history(error_message)
#             return state

#     @staticmethod
#     def _format_classification_results(results: Dict[str, float]) -> str:
#         """
#         Format classification results concisely.
#         """
#         if not results:
#             return "Kh√¥ng c√≥ k·∫øt qu·∫£ ph√¢n lo·∫°i."

#         return "\n".join(f"- {label}: {confidence:.2f}"
#                         for label, confidence in results.items())

#     @staticmethod
#     def _format_search_results(results: List) -> str:
#         """
#         Format search results concisely.
#         """
#         if not results:
#             return "Kh√¥ng c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm."

#         formatted_results = []
#         for doc in results[:3]:  # Limit to top 3 results
#             formatted_results.append(f"{doc.page_content.strip()}")

#         return "\n\n".join(formatted_results)

#     def process_query(self, query: str, image_path: Optional[str] = None):
#         """
#         Process query with optimized workflow.
#         """
#         print(f"ü§ñ PROCESS QUERY: Starting at {datetime.now().isoformat()}")
#         # Create new state from default
#         if isinstance(self.default_state, dict):
#             current_state = AgentState(**self.default_state)
#         else:
#             current_state = self.default_state.model_copy(deep=True)
#         current_state.query = query
#         current_state.image_path = image_path

#         # Execute workflow
#         result = self.workflow.invoke(current_state)

#         # Update default state
#         self.default_state = result.copy()

#         print(f"\nü§ñ PROCESS QUERY: Completed at {datetime.now().isoformat()}")
#         return result['final_answer']